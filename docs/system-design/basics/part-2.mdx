---
title: Load Balancer
description : "Systems Design Basics"
sidebar_position: 2
---

### What is Reliability?
Reliability is the probability that a system will fail in a given period.
An often quoted reliability standard is the "five nines" (99.999 percent) reliability. This allows for 5.26 minutes of downtime per year.
A Distributed system will be considered reliable it it is able to provide the same service even if one or more of its components fail.
Any failing node can be replaced with a new node without affecting the service.

### What is Availability?
Availability is the time a system remains operational to perform its required function in a specific period.
Availability is usually measured as a percentage of uptime in a given year.
Availability = uptime / (uptime + downtime)
A system that is highly available will be able to provide service even if one or more of its components fail.
Any failing node can be replaced with a new node without affecting the service.

### What is Scalability?
Scalability is the ability of a system to handle increased load by adding resources to the system.
A system that scales well will be able to maintain or even increase its level of performance or service even as it gets larger.
Scalability can be measured in terms of latency or throughput.
Latency is the time taken to respond to a request.
Throughput is the number of requests a system can handle per second.

### What is Load Balancing?
Load balancing is the process of distributing network traffic across multiple servers.
Load balancing improves responsiveness and increases availability of applications, websites or databases.
Load balancing is performed by a dedicated software or hardware such as a multilayer switch or a Domain Name System server.
Load balancing can be implemented with hardware, software or a combination of both.
Load balancing can be implemented at the network level, transport level and application level.
Load balancing can be implemented with a single server or multiple servers.
Load balancing can be implemented with a single data center or multiple data centers.
Load balancing can be implemented with a single cloud provider or multiple cloud providers.

### What is a Load Balancer?
A load balancer is a device that acts as a reverse proxy and distributes network or application traffic across a number of servers.
- Load balancers are used to increase capacity (concurrent users) and reliability of applications.
- Load balancers can be implemented with hardware (expensive) or with software such as HAProxy, nginx or Apache (open source).
- Load balancers can be configured to check for failed servers and automatically remove them from the pool temporarily.
- Load balancers can also be configured to balance traffic across multiple data centers.
- Load balancers can be configured to use multiple algorithms to determine how to distribute traffic.
- Load balancers can route traffic based on various factors, including:
    - Random
    - Least loaded
    - Session/cookies
    - Round robin or weighted round robin
    - Layer 4
    - Layer 7
